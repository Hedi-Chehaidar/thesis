% !TeX root = ../main.tex
% Add the above to each chapter to make compiling the PDF easier in some editors.

\chapter{Related Work}\label{chapter:related work}

% Mihail: "Block-Based Compressors". Use this webpage to make sure it's always consistently capitalized:
% https://capitalizemytitle.com.
\section{Block-based Compressors}
General-purpose block-based compression algorithms are widely used across storage systems, operating systems, and data processing pipelines due to their ability to achieve high compression ratios on diverse data types. These compressors operate by partitioning the input data into blocks of fixed or variable size and compressing each block independently. This design enables parallel compression and decompression, robust error containment, and adaptability to different data distributions.

\subsection{LZ4}
One of the most prominent examples of such algorithms is LZ4, a lightweight compressor based on the Lempel–Ziv 77 (LZ77) family of dictionary compression techniques.
The LZ77 general technique works by scanning a block with a usual size of 128 KB as input, detecting byte sequences that appeared before. Repeated sequences are encoded as a

% Mihail: I prefer `\texttt{offset}, \texttt{length}' - looks much better.
pair $(offset, length)$ where $offset$ references the first appearance of the sequence in the output stream.

What distinguishes LZ4 is that during compression a hash table is used to quickly identify matches in the input and greedily accepting the first match found. This approach minimizes CPU usage, providing compression speed > 500 MB/s per core.

LZ4 decompression is very similar to just performing memory copy ('memcpy') operations, making it bounded by memory bandwith with speed of multiple GB/s per core, tipically reaching RAM speed limits on multi-core systems.
 
LZ4 prioritizes decompression speed over compression ratio, making it suitable for performance-critical workloads where fast data access is required. It achieves compression by identifying repeated byte sequences within a block and encoding them as references to earlier occurrences \cite{lz4}.

While LZ4 offers very fast decompression, its relatively small window size and simple matching strategy limit its ability to exploit longer-range redundancies, especially across block boundaries.

\subsection{Zstd}
Zstandard (Zstd) \cite{zstd} represents a more advanced block-based compression approach developed by Facebook that balances compression ratio and performance. Zstandard combines LZ-style dictionary matching with entropy coding and supports large compression windows, allowing it to capture long-distance repetitions within a block.
Zstd encodes unmatched byte sequences, called literal bytes, in addition to the offset and length attributes of match pairs using Finite State Entropy (FSE) and Huffman coding techniques.
As a result, Zstandard often achieves significantly higher compression ratios than lightweight compressors such as LZ4, particularly on structured or repetitive data. However, this improved compression comes at the cost of increased decompression complexity and reduced random-access efficiency, as individual values typically cannot be decompressed without processing the surrounding block.

% Mihail: We don't need this. Since we don't introduce snappy, quicklz etc. Maybe you can write in text the compression & decompression time of the ones you explained (LZ4 and Zstd), but having a table with *all*, it really doesn't bring anything :)
\begin{table}[htbp]
\centering
\caption{Comparison of different block-based compressors with the Silesia compression corpus \cite{zstd,silesia}}
\label{tab:compression-performance}
\begin{tabular}{lcccc}
\hline
Compressor name & Ratio & Compression & Decompress. \\
\hline
zstd 1.5.6 -1         & 2.887 & 510 MB/s & 1580 MB/s \\
zlib 1.2.11 -1        & 2.743 & 95 MB/s  & 400 MB/s  \\
brotli 1.0.9 -0       & 2.702 & 395 MB/s & 430 MB/s  \\
zstd 1.5.6 --fast=1   & 2.437 & 545 MB/s & 1890 MB/s \\
zstd 1.5.6 --fast=3   & 2.239 & 650 MB/s & 2000 MB/s \\
quicklz 1.5.0 -1      & 2.238 & 525 MB/s & 750 MB/s  \\
lzo1x 2.10 -1         & 2.106 & 650 MB/s & 825 MB/s  \\
lz4 1.9.4             & 2.101 & 700 MB/s & 4000 MB/s \\
lzf 3.6 -1            & 2.077 & 420 MB/s & 830 MB/s  \\
snappy 1.1.9          & 2.073 & 530 MB/s & 1660 MB/s \\
\hline
\end{tabular}
\end{table}


\subsection{Limitations}
While block-based compressors are highly effective for bulk storage and sequential access patterns, their design fundamentally limits random access performance. In analytical database systems, queries often access individual values or small subsets of columns rather than scanning entire blocks. In such scenarios, block-based compression requires decompressing full blocks even when only a small portion of the data is needed, leading to unnecessary computation and memory traffic. This limitation has been highlighted in prior work on column-oriented storage and analytical query processing, where fine-grained access and low-latency decompression are critical for performance.

Several systems attempt to mitigate this issue by using smaller block sizes or by combining block-based compression with dictionary encoding. However, reducing block size typically degrades compression efficiency, while hybrid approaches increase system complexity. As discussed in the FSST paper, these trade-offs make block-based compressors less attractive for scenarios where fast random access to compressed strings is a primary requirement \cite{fsst}.

Consequently, lightweight compression schemes that avoid block-level dependencies have gained attention in the context of analytical databases. These schemes trade some compression efficiency for predictable decompression costs and fine-grained access. FSST belongs to this class of compressors, providing symbol-based compression that allows individual strings to be decompressed independently. Understanding the strengths and limitations of block-based compressors is therefore essential for positioning FSST and the improvements proposed in this thesis relative to existing compression techniques.

\section{FSST}

\subsection{General Idea}

Fast Static Symbol Table (FSST) \cite{fsst} is a lightweight string compression algorithm that works by replacing substrings (symbols) in the original data with one-byte codes. A symbol table is constructed from a sample of the data, mapping symbols to codes. The term “static” refers to the fact that any string can be compressed or decompressed independently using the symbol table, without requiring any prior context, unlike block-based compressors discussed in the previous section.
Individual strings in databases are typically no larger than 200 bytes, with the majority being

% Mihail: This needs a citation.
less than 30 bytes. This makes the random-access property of FSST more valuable compared to block-based compressors, which often need to decompress an entire block of 128 KB to retrieve a single string. FSST also supports compressed query processing, since equality between original strings is equivalent to equality between their compressed representations. It may also be possible to perform more complex operations on compressed strings, such as pattern matching, which can reduce the need to decompress strings when processing a wide range of queries.

% Mihail: Do you explain the figure? 
\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.8\textwidth]{figures/fsst_example}
  \caption{FSST compression example \cite{fsst}}
  \label{fsst_example}
\end{figure}

Symbols have lengths ranging from 1 to 8 bytes, and each symbol is mapped to a code from 0 to 254, with code 255 reserved as an escape code that plays an important role during decompression. Consequently, the total number of usable symbols is 255. This results in a worst-case symbol table memory overhead of $8 \times 255 + 255$ bytes, which is very unlikely in practice, as many symbols tend to be only 2 to 3 bytes long.

\subsection{Decompression}
FSST decompression is fairly simple. The algotithm

% Mihail: typo.
ieterates over the encoded input stream

% Mihail: byte by byte? Or is byte for byte correct?
byte for byte and appends the corresponding symbol from the symbol table to the output stream. The exception of the escape code (code 255) can be checked with an

% Mihail: I'd put if as `texttt{if}' to look better. If not, someone cannot parse it, since it's an English word.
if statement, where in that case the algorithm appends the following raw byte after the escape code as is and continues with the next byte.

The original FSST open-source implementation \cite{fsst_repo} optimizes decoding to avoid performing an if statement for every byte and instead loads 4 bytes every time and then checks if they contain the escape code. FSST decoding is therefore comparably fast approaching 2 GB/s according to the original paper's evaluation.

The improvements of this thesis do not affect the FSST desompression method in any way. Decompression was used in this thesis solely to

% Mihail: robustness? Maybe correctness?
verify robustness of the added contributions.

\subsection{Compression}

FSST compression is performed independently for each string in the corpus. A loop iterates over the bytes of the string. In each iteration, the longest matching symbol starting at the current position is used for encoding, and the current position in the string is updated accordingly. If no symbol in the table matches the current position, the byte is escaped by appending the escape code to the output stream, followed by the raw byte from the string.

The function used to find the longest matching symbol is called $findLongestSymbol$, and its implementation is shown in Figure~\ref{algorithm3}. The same function is also used during symbol table construction.

This greedy approach enables fast encoding at the cost of a suboptimal compression factor. An alternative approach is presented in Section~\ref{dp}.

\subsection{Symbol Table Construction}

The construction of the symbol table is the most important component of the FSST algorithm with respect to the achieved compression factor, as the encoding quality is tightly dependent on the set of symbols selected for the table. Selecting symbols is challenging because their effectiveness depends on one another whenever they overlap. This is the dependency issue described in the original paper.

% Mihail: Don't start a sentence with "So".
So the only reliable way to assess the significance of a symbol is to compress the sample using the current symbol table. However, evaluating all possible symbol tables is not feasible due to the enormous number of possibilities 
($\binom{8N}{255}$ where $N$ denotes the sample size).

The FSST algorithm shown in Figure~\ref{algorithm3} constructs the symbol table by iterating over a sample of the corpus for a fixed number of generations. In each generation, the current table is refined by discovering new symbol combinations and retaining those with the highest static gain. The gain of a symbol is computed as $gain = length \times frequency$, where length is the symbol length (between 1 and 8 bytes) and frequency is the number of times the symbol is selected when compressing the sample using the previous table.

In the original implementation, the number of generations is set to five and the sample size to 16 KB. The sample is processed incrementally, such that in each generation a larger fraction of the sample is considered for symbol selection.

% Mihail: This is too small. We could split into 2 columns - and we should use listing, in any case.
% Mihail: Do you really need all the functions outlined in this pseudocode? Just keep the ones that you really need.
\begin{figure}[h]
  \centering
  \includegraphics[width=0.3\textwidth]{figures/algorithm3}
  \caption{FSST initial idea symbol table construction \cite{fsst}}
  \label{algorithm3}
\end{figure}

% Mihail: I'd suggest we make all the functions either with \textsc or \texttt - otherwise it looks very simple.
The buildSymbolTable function is the main component of the algorithm, in which the five iterations over the sample are performed. Two counters,  $count1$ and $count2$, are used to record symbol frequencies when compressing the text using the previous symbol table. The counter $count1$ tracks individual symbols, while $count2$ records the concatenation of two symbols or the concatenation of a symbol and a literal byte.

The indices of these counters range from 0 to 511. Indices from 0 to 255 correspond to escaped bytes (literal bytes), while indices from 256 to  $256 + st.nSymbols - 1$ (up to 511) denote actual symbols.

In each generation, the counters are populated using the compressCount function, which compresses the text with the current symbol table. This function counts individual symbols in 
$count1$ and records all pairs of successive symbols, as well as combinations of a symbol followed by a literal byte, in $count2$.

Real symbols are stored in $st.symbols$ starting from index 256 in alphabetical order. Symbols that begin with the same character are sorted in decreasing order of length. As a result, $st.findLongestSymbol$ terminates after finding the first matching symbol. The implementation details of this function in the open-source code are discussed later.

A new symbol table is then constructed from the counters using the makeTable function. Each symbol, literal byte, and their concatenations are inserted into a max heap, where the key is the computed gain of the candidate symbol. The 255 symbols with the highest static gains are selected for the next symbol table. Finally, the makeIndex function sorts the symbols as described above and initializes the $st.index[letter]$ table, which stores the position (code) of the first symbol beginning with a given letter.
\newline

\noindent
% Mihail: I usually do a paragraph title here.
% I use the following command.
% Custom paragraph.
% \newcommand{\sparagraph}[1]{\vspace{1mm}\noindent {\bf #1}}
% And then use like:
% \sparagraph{Evolution.}
% If you don't like this, you can also use \subpargraph{} or \paragraph{}. See an example here, what it looks like: https://arxiv.org/pdf/2601.00768. 
The FSST algorithm evolved from the previously described version to a branchless implementation that enables the use of SIMD instructions (AVX-512), significantly reducing encoding time. The primary bottleneck in encoding is the $findLongestSymbol$ function. Therefore, a redesign of the symbol storage was introduced as a first optimization.

Symbol codes are now stored in two separate data structures: a $256 \times 256$ array named $shortCodes$ for symbols of length one and two bytes, and a hash table $hashTab$ of size 1024 (1 KB) for symbols with lengths ranging from three to eight bytes. A lossy perfect hashing scheme is employed for longer symbols, using a hash function based on the first three bytes of the symbol. If inserting a symbol would result in a collision, the symbol is discarded, retaining only the symbol with the highest gain among colliding symbols of length three or greater.

For each character $A$ for which no entry in $shortCodes[A][*]$ is defined, the corresponding single-byte symbol is inserted if it exists. This guarantees that an access to $shortCodes$ always returns the longest matching symbol of length one or two. The $findLongestSymbol$ function first queries the $hashTab$ to search for a matching symbol, then consults $shortCodes$ if no match is found, and finally falls back to returning the literal byte. The function’s return value is computed using a conditional move (MOV) instruction, thereby avoiding branching.
\newline

\noindent
Although the original FSST algorithm is highly optimized for runtime, particularly when using the SIMD variant, the greedy strategy employed by the $findLongestSymbol$ function does not always select the optimal symbol, both during compression and during symbol table construction. This leaves room for improving the compression factor by adopting a different optimization strategy that more carefully considers conflicts between overlapping symbols, as discussed in the following section.

\begin{comment}
Citation test~\parencite{latex}.

Acronyms must be added in \texttt{main.tex} and are referenced using macros. The first occurrence is automatically replaced with the long version of the acronym, while all subsequent usages use the abbreviation.

E.g. \texttt{\textbackslash ac\{TUM\}, \textbackslash ac\{TUM\}} $\Rightarrow$ \ac{TUM}, \ac{TUM}

For more details, see the documentation of the \texttt{acronym} package\footnote{\url{https://ctan.org/pkg/acronym}}.
\subsection{Subsection}

See~\autoref{tab:sample}, \autoref{fig:sample-drawing}, \autoref{fig:sample-plot}, \autoref{fig:sample-listing}.

\begin{table}[htpb]
  \caption[Example table]{An example for a simple table.}\label{tab:sample}
  \centering
  \begin{tabular}{l l l l}
    \toprule
      A & B & C & D \\
    \midrule
      1 & 2 & 1 & 2 \\
      2 & 3 & 2 & 3 \\
    \bottomrule
  \end{tabular}
\end{table}

\begin{figure}[htpb]
  \centering
  % This should probably go into a file in figures/
  \begin{tikzpicture}[node distance=3cm]
    \node (R0) {$R_1$};
    \node (R1) [right of=R0] {$R_2$};
    \node (R2) [below of=R1] {$R_4$};
    \node (R3) [below of=R0] {$R_3$};
    \node (R4) [right of=R1] {$R_5$};

    \path[every node]
      (R0) edge (R1)
      (R0) edge (R3)
      (R3) edge (R2)
      (R2) edge (R1)
      (R1) edge (R4);
  \end{tikzpicture}
  \caption[Example drawing]{An example for a simple drawing.}\label{fig:sample-drawing}
\end{figure}

\begin{figure}[htpb]
  \centering

  \pgfplotstableset{col sep=&, row sep=\\}
  % This should probably go into a file in data/
  \pgfplotstableread{
    a & b    \\
    1 & 1000 \\
    2 & 1500 \\
    3 & 1600 \\
  }\exampleA
  \pgfplotstableread{
    a & b    \\
    1 & 1200 \\
    2 & 800 \\
    3 & 1400 \\
  }\exampleB
  % This should probably go into a file in figures/
  \begin{tikzpicture}
    \begin{axis}[
        ymin=0,
        legend style={legend pos=south east},
        grid,
        thick,
        ylabel=Y,
        xlabel=X
      ]
      \addplot table[x=a, y=b]{\exampleA};
      \addlegendentry{Example A}
      \addplot table[x=a, y=b]{\exampleB};
      \addlegendentry{Example B}
    \end{axis}
  \end{tikzpicture}
  \caption[Example plot]{An example for a simple plot.}\label{fig:sample-plot}
\end{figure}

\begin{figure}[htpb]
  \centering
  \begin{tabular}{c}
  \begin{lstlisting}[language=SQL]
    SELECT * FROM tbl WHERE tbl.str = "str"
  \end{lstlisting}
  \end{tabular}
  \caption[Example listing]{An example for a source code listing.}\label{fig:sample-listing}
\end{figure}
\end{comment}