% !TeX root = ../main.tex
% Add the above to each chapter to make compiling the PDF easier in some editors.

\chapter{Evaluation}\label{chapter:evaluation}

In this section we will discuss the improvement of the three introduced contributions on the original FSST algorithm in terms of compression factor and runtime across different datasets. The contributions will be also tested against other compression techniques that use FSST, like DICT FSST \cite{dict_fsst} and FSST+ \cite{fsst_plus_repo}.

\section{Benchmarking Data}

\subsection{Dbtext}

This is the dataset used for benchmarking in the original FSST paper~\cite{fsst}. It includes real-world text data from different sources. The data consists of 23 string columns ranging from 130~KB (city) to around 6~MB (urls) in size. They can be grouped in five categories based on their data nature:
\begin{itemize}
  \item machine-readable identifiers: hex, yago, email, wiki, uuid, urls2, and urls.
  \item human-readable names: firstname, lastname, city, credentials, street, and movies.
  \item text: faust, hamlet, chinese, japanese, and wikipedia.
  \item domain-specific codes: genome and location
  \item TPC-H data: c\_name, l\_comment and ps\_comment
\end{itemize}

\subsection{NextiaJD}

NextiaJD~\cite{nextiajd} has a collection of diverse string columns, dedicated to learning-based research. The selected tables from NextiaJD are:
\begin{itemize}
  \item github-issues: it has the largest column of the whole benchmarking data (body) with 32~MB size.
  \item glassdoor: the table with the most string columns, the majority being urls and path links. 
  \item Homo\_sapiens.GRCh38.92: mostly duplicate heavy data (identifiers).
  \item Parking\_Violations\_Issued\_Fiscal\_Year\_2017: containing street names and dates.
  \item Reddit\_Comments\_7M\_2019: the majority of columns are usernames, but also contains one text column (body) and one column with links (permalink)
  \item reviews\_detailed: it combines date data with large text data.
\end{itemize}

\subsection{PublicBIbenchmark}

PublicBIbenchmark is a dataset produced by CWI (Centrum Wiskunde \& Informatica) with data representing real-world business intelligence workloads \cite{publicbibenchmark}. The data selected combine geographical locations, urls with prefix-heavy patterns, and user-generated data in different languages and special characters (hashtags) to test the affect of the improvements with heterogenous data. The tables chosen from this dataset are ``HashTags/Hashtags1'', ``IGlocations2/IGlocations2\_2'', and ``YaleLanguages/YaleLanguages\_1''. 


\subsection{CyclicJoinBench}

CyclicJoinBench is a dataset used to evaluate join algorithms on cyclic query graphs~\cite{cyclicjoinbench}. The datasets contains three identical string containing tables regarding content, therefore only one table was used for benchmarking (SNB1-parquet). Most of the string columns of this dataset has short rows featuring names, titles, and locations.


\subsection{ClickBench}

ClickBench is a dataset developed by ClickHouse~\cite{clickbench}, used to benchmark the performance of over 50 databases on simple analytical workloads. This makes the dataset suitale for the evaluation of compression methods. ClickBench has only three string type columns that were all used for benchmarking: Title, Referer, and URL.


\section{Benchmarking Method}

A filtering process was applied on the selected tables from the datasets mentioned above in order to get string columns suitable for benchmarking the affects of the improvements. The filtering conditions are the same used for benchmarking FSST+, mentioned in the corresponding thesis~\cite{fsst_plus}.

First, each relation is reduced to its first 100~K rows. Then, to eliminate columns with numeric values, the only allowed column type was VARCHAR. Furthermore, only columns having an average row length $\ge 8$ were retained. An exception is the dbtext dataset that was taken fully as is, although it had a column with an average length of 7 (firstname). 

An additional cndition was applied, that was not used for benchmarking FSST+, setting an upper-bound of 35~MB for the resulting text file of each column, in order to eliminate huge files (outliers) that skewed the average runtime measure. 
\newline
\sparagraph{DICT FSST fairness.}To ensure a fair comparison with DICT FSST, the resulting columns should not be dominated by duplicates. Otherwise, the DICT FSST compression will be mostly influenced by the dictionary encoding, which consists of keeping a dictionary to map each row to its value in a duplicate-free version of the column. The second part of the DICT FSST is the more relevant part for the comparison, as it uses FSST to compress the unique columns, i.e, the contributions of the thesis are only applicable in this part. As a result, only columns whose FSST compression size is not more than twice the size of dictionary encoding are retained. 

The condition used is then $\text{FSST compressed size} \le 2 \times \text{Dictionary encoding size}$. The size of the dictionary is calculated as the sum of the lengths of unique strings and the total size of tha integers used for mapping, according to the SQL query of Lst.~\ref{dict}.

\begin{lstlisting}[language=SQL, caption={SQL query to get dictionary encoding size.}, label={dict}]
SELECT
  CAST(
    COALESCE(length(string_agg(DISTINCT col, '')), 0) -- length of unique strings
    +
    (COUNT(col) * CASE -- total number of strings * bytes used to map each string
        WHEN COUNT(DISTINCT col) = 0 THEN 0
        ELSE ceil(log2(COUNT(DISTINCT col)) / 8) -- bits / 8 to count bytes
      END
    )
  AS BIGINT) AS total_compressed_size
  FROM rel
\end{lstlisting}


\section{Results}

\subsection{Ablation Study}

We are going to discuss the effect of the contributions of this thesis (DP, third counter, and pruning) on the FSST algorithm regarding compression factor (CF). The resulting codebase~\cite{btrfsst_repo} is an extension of the original FSST code~\cite{fsst_repo}, allowing the adjustment of the included imrovements with command line options as follows:

\begin{itemize}
  \item no options : Exactly FSST will be run
  \item --dp-train : The DP function will be used for building the symbol table instead of the greedy \texttt{findLongestS>ymbol}
  \item --dp-encode: The DP function will be used for compressing the whole corpus after constructing the symbol table.
  \item --triples  : \texttt{compressCount} will use three counters instead of two.
  \item --prune    : Pruning will be used in the filling of the new symbol table in each generation in \texttt{makeTable}.
\end{itemize}

Running the basecode with with any configuration different than FSST (i.e, with at least one of the options above) will discard many of the heuristics used by the original FSST code. These heuristics include a $\times 8$ boost to the gain of one-byte symbols, using fractions of the sample incrementally across generations instead of training on the whole sample for the 5 generations, and setting an entry threshold for the count of the symbols before pushing them to the heap of candidates, that depends on the fraction used of the sample.

The program running with all the above mentioned options will be called ``BtrFSST'', the name of all improvements combined with FSST as a base.

\begin{figure}[!htbp]
  \centering
  \includegraphics[width=1\textwidth]{figures/cf\_combined}
  \caption{Effect of improvements on the compression factor over all datasets combined.}
  \label{cf_combined}
\end{figure}

Figure~\ref{cf_combined} shows the CF of all the columns of the used datasets over different configurations. The left side adds the improvements, affecting only the symbol table construction (the compression is greedy). The right side always includes DP on compression and then adds the contributions gradually in the same way.

The overall CF average improved from FSST to BtrFSST by 8.3\%. Without including DP on compression, the contributions improved the average CF by 3.2\% by solely improving the symbol table construction, while the improvement was by 3.4\% when DP was used on compression. This shows that not using the greedy \texttt{findLongestSymbol} at all has a stronger impact on the overall CF. The implementation backs this claim, as when DP is used on both table construction and compression, hshing will be dropped for symbols. That means, in that case all symbols selected will be used in the symbol table, without the possibility for collisions, which was certain for symbols sharing the first three bytes, the key for the hash function in FSST.  

The DP approach in the compression has the highest effect of the average CF with an improvement of 4.7\% over FSST with greedy encoding. Then comes DP on table construction (2.4\% with DP on compression and 2.5\% without), which shows that the greedy approach was the main reason behind the sub-optimal CF of the original FSST. ALthough adding a third counter and using symbol pruning during symbol table filling did not have significant effects on the overall CF means, they can be still helpful for some workloads. We can see, in the boxenplot for adding a third counter when using DP on training and encoding (right side), that this configuration had the highest compression factors for some columns.
\newline

\begin{figure}[!htbp]
  \centering
  \includegraphics[width=1\textwidth]{figures/time\_combined}
  \caption{Effect of improvements on the runtime over all datasets combined.}
  \label{time_combined}
\end{figure}

\noindent
Figure~\ref{time_combined} shows the runtimes with the different configurations over all columns. The machine used for this benchmarking is a laptop equipped with an AMD Ryzen 9 CPU with 8 cores and 16 threads, operating at a base frequency of 3.3~GHz. The system uses 16~GB of RAM and an integrated AMD Radeon Graphics, with storage being provided by an NVMe SSD. All benchmarks were executed with the device connected to external power and configured in high-performance mode. No SIMD code was executed, meaning that the compression method used for original FSST was the scalar one and not with AVX-512, which should be $\times 2.5$ faster~\cite{fsst}. 

The benchmarks show that BtrFSST is on average almost twice slower than FSST. The addeed runtime overhead comes primarily from DP on encdoing, which alone increased average runtime by 80\%. All contributions affecting the symbol table construction made the algorithm 31.8\% slower than FSST, but only 10.2\% comparing to FSST with DP on encoding. This proves that the bottleneck is compression, especially with columns that are significantly larger than the sample size (16~KB). 

\subsection{Special Cases}

We will dive deeper into the CF comparison of FSST and BtrFSST (all contributions on table construction and encoding), by exploring the best and worst columns for both methods.